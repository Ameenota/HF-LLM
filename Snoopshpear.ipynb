{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ameenota/HF-LLM/blob/main/Snoopshpear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfqZF84uBzup"
      },
      "source": [
        "# The Shakespear & Snoop colab no-one asked for"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we will build a Causal Language Model (CLM), trained on the works of Snoop Dogg & William Shakespear. In theory you can just prompt an LLM to do this, but our expectation is this will hallucinate less and is obviously a *lot* smaller in size. Training a CLM on specific data builds domain expertise on a model. Practical uses of such models can be training it on a specific type of text (e.g. Medical Research) and then use that model to either generate compliant text or use transfer learning to even detect fraudulent papers. For now, we will focus on *next* token prompting."
      ],
      "metadata": {
        "id": "RBag9vhbAGuQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL2-J02sBzuq"
      },
      "source": [
        "Lets install some initial libraries to start building this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O5kCGd7zBzuq"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece] -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2jl7x_yBzur"
      },
      "source": [
        "To skip the tedious part of collecting our input data, I built a dataset on HuggingFace on the [works of Shakespear and Snoop](https://huggingface.co/datasets/sagsan/snoopshpear). This was simply pulled from online resources, cleaned up a bit using pandas and uploaded to HF. The only trick here is that Shakespear has a *lot* more published text (10x in my data). So, I up-sampled Snoops works. Up-sampling simply mean duplicating it until we get to similar sizes for both our labels and it solves the problem of an im-balanced dataset. Feel free to head over [to HF to play](https://huggingface.co/datasets/sagsan/snoopshpear) around with the data. Lets load up our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDj33mTCBzus",
        "outputId": "6c8cf1d6-b8ff-4255-a10b-4d529772614a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'source'],\n",
              "        num_rows: 125107\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['text', 'source'],\n",
              "        num_rows: 13901\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "raw_datasets = load_dataset(\"sagsan/snoopshpear\")\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see we have around 221K rows in our train set and 24K in our validation set. We also have 2 columns \"text\" & \"source\". Lets peek at our data"
      ],
      "metadata": {
        "id": "bwKSzNmoE4On"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jtRQqBKzBzut",
        "outputId": "c3112e1f-4777-4ba2-f645-398b16c07041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: shakespear\n",
            "Text: Think you 'twere prejudicial to his crown? No; for he could not so resign his crown\n",
            "\n",
            "Source: shakespear\n",
            "Text: Thy valour and thy heart- thou art a traitor;\n",
            "\n",
            "Source: snoop\n",
            "Text: Get a ketchup, get 'em messed up, turn the heat up let 'em fry\n",
            "\n",
            "Source: shakespear\n",
            "Text: Their kind acceptance weepingly beseeched,\n",
            "\n"
          ]
        }
      ],
      "source": [
        "start_id = 69\n",
        "end_id = 73\n",
        "text, source = raw_datasets[\"train\"][start_id:end_id]['text'], raw_datasets[\"train\"][start_id:end_id]['source']\n",
        "for i in range(end_id - start_id):\n",
        "  print(f\"Source: {source[i]}\\nText: {text[i]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# during development you want to work with smaller datasets\n",
        "#raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(10000)) ## delete this.\n",
        "#raw_datasets\n"
      ],
      "metadata": {
        "id": "DhfcDdczhT9b"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, each row of our data is either a line by Snoop or Shakespear. To train a CLM we don't really need the source. We will just use all the text fields as inputs to our model."
      ],
      "metadata": {
        "id": "3Zr-IXPYGlfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEWLINE_TOKEN = \"<|im_end|>\"\n",
        "\n",
        "def append_newline_token(example):\n",
        "    example[\"text\"] = example[\"text\"].strip() + \"\\n\" + NEWLINE_TOKEN\n",
        "    return example\n",
        "\n",
        "formatted_datasets = raw_datasets.map(\n",
        "    append_newline_token,\n",
        "    num_proc=4, # Use multiple processes for faster execution\n",
        ")\n",
        "\n",
        "# Example verification of the first row (the output will show the new token at the end)\n",
        "print(f\"First training example after append:\\n'{formatted_datasets['train'][0]['text']}'\")\n",
        "raw_datasets = formatted_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "bea416a2c40442b99117d3f69a75ff7c",
            "f133cfddc0d14e3688f6e1a400902c4b",
            "9fd55bfb36d5494abb15302785790228",
            "f20b9c0fdc7b48c39b879e6ed923d271",
            "f3a8c1ff7b2c4fd5a3c786509418946e",
            "8078745300a14a5bb63be165a8ab7cc8",
            "cf2c50affcca4aeeb1e88c28f41b355e",
            "42871a9383c34b4dad6ae773da0e4395",
            "9699df0602be49798cbc5e2298ef2f36",
            "27abf1288494497ca25323408ee2fe57",
            "10992af4c59a4b2980cfab1e3ee2bef9"
          ]
        },
        "id": "N9vTU_-Sgi49",
        "outputId": "33d4bf0f-f74e-446d-aea1-21ab0b1c0ba6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/125107 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bea416a2c40442b99117d3f69a75ff7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First training example after append:\n",
            "'Cause ain't nuttin but sweat inside my hand\n",
            "<|im_end|>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kind of a weird thing I discovered after building this is that because our input data does not have newlines, our model was outputting text without any space between line. To fix this we can add a custom token at the end of every input of our dataset. A bit later we will tell the tokenizer & model about this custom token. This fixes of no spacesbetweensentences."
      ],
      "metadata": {
        "id": "kiDeUMLClpxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok as with most practical AI projects we will re-use an existing model instead of building one from scratch. DistilGPT2 is a smaller and faster version of GPT2 which makes it ideal for our use case.\n",
        "\n",
        "Our first step is to tokenize the input text. Tokenization as you may have heard in LLMs is what breaks sentences down to smaller pieces and converts them to numbers that our model can understand. There are many ways to tokenize, however we use the AutoTokenize class and pass in DistilGPT2 as the model. This ensures that we tokenize in exactly the same way as how the model was trained.\n"
      ],
      "metadata": {
        "id": "ZVaBgDrXZTQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rYVs8SQDBzut",
        "outputId": "d7b866a0-efff-409c-c103-3d14b6229e86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs length: 3\n",
            "Input chunk lengths: [12, 9, 13]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_CHECKPOINT = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "context_length = 512\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "test_outputs = tokenizer( #Tokenize sample text\n",
        "    raw_datasets[\"train\"][:3][\"text\"], #run a quick check with just 3 inputs\n",
        "    return_length = True\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(test_outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(test_outputs['length'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ibKemse_qYv6",
        "outputId": "04725c64-3534-4a21-ab37-6c879027fa9e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_end|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Our sentences can be of different lengths, for computational efficiency our model performs much better if our inputs are the same length.\n",
        "1. First we setup our context_length to 1024. This specifies the number of tokens our model processes at once and matches GPT2s maximum context window.\n",
        "\n",
        "2. Then we specify a pad_token. This token is simply added to short sentences to bring them up to the same size. The model will ignore the pad_token. GPT2 models do not have a pad token so we re-use the end-of-string token for this. The EOS token on GPT2 is \"<|endoftext|>\"\n"
      ],
      "metadata": {
        "id": "VLW2f8ESbWGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sz_39NGG1Em",
        "outputId": "5524e1e8-9ad5-4333-a53a-8250b02e9d67"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[60912, 36102, 944, 9979, 55971, 714, 27466, 4766, 847, 1424, 198, 151645], [64117, 661, 13, 24233, 11, 847, 36931, 624, 151645], [18284, 2363, 13, 1674, 473, 11, 27048, 11, 566, 374, 12796, 4894, 151645]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'length': [12, 9, 13]}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets talk about the outputs of the tokenizer on our 3 input strings above\n",
        "1. input_ids: This is our main tokenized data and is the numeric representation of our text.\n",
        "2. attention_mask: This mask specifies which tokens to ignore in our input_ids (e.g. padding tokens will be set to 0 in the attention_mask)\n",
        "3. length is just the length of each string."
      ],
      "metadata": {
        "id": "c-FnxhLQcNtH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bN_Ph9bdBzut",
        "outputId": "0b3d070f-0429-43cf-9c8a-b2d9b3af4c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "04b7a82085bb4f0ca7c8f530951d1588",
            "eedf977a3a504e8a9f228f6c090413bd",
            "629a2b25a7ac450aa946688dc0fe1e51",
            "68bc957dfa174910ad233ab5acf4ad4b",
            "36cb3b3cb38341879912e21138243baa",
            "1b5f9fae9f484756b09b8f3d121c7e83",
            "b00e1d6b517d40428e75b4b56bef878d",
            "e35dd3d400a043dca8a4524a7a5cb5dc",
            "1b6f4e94583940678657c8665cba6d60",
            "ab056cbae56f435387f837cc674c7620",
            "126cb3716c0244e3a79973be5ed777e9"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/125107 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04b7a82085bb4f0ca7c8f530951d1588"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 125107\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 13901\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "def tokenize(examples):\n",
        "  return tokenizer(examples[\"text\"])\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok finally the above snippet runs tokenization on the entire dataset. This is exactly the same as what we did above. We do enable batching to speed up the process and we also remove all the input columns as we don't need them any longer."
      ],
      "metadata": {
        "id": "tuy7UP70dc24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets consider what our input_ids look like\n",
        "```\n",
        "'input_ids':  [[5188, 33, 11262, 16868, 13, 4162, 314, 534, 27517, 30],\n",
        "[261, 11, 8011, 319, 13],\n",
        "[33, 16696, 2751, 33363, 13, 3412, 326, 11, 314, 2911, 11, 543, 21289, 2788, 1793, 2029, 11]\n",
        "...]\n",
        "```\n",
        "Each of the lists above represents a sentence. As mentioned above we want to send 1024 tokens at a time to our model. Transformer models like GPT2 use positional embedding. As our model is limited to 1024, sending more tokens will cause it to crash. Using a smaller size is very inefficient as the sequence must be extended to 1024 to make use of GPU parallel processing.\n"
      ],
      "metadata": {
        "id": "FJ9AO5viYeGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate all texts from the batch into a single stream\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "\n",
        "    #Drop the last, incomplete batch of tokens (if it exists)\n",
        "    total_length = (total_length // context_length) * context_length\n",
        "\n",
        "    # Split the stream into fixed-size chunks of size 1024\n",
        "    result = {\n",
        "        k: [t[i : i + context_length] for i in range(0, total_length, context_length)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "\n",
        "    #Create the labels (for CLM, the input IDs are the targets, shifted internally)\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "\n",
        "# Apply the grouping function to the tokenized datasets\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000, # Grouping benefits from a large batch size\n",
        "    num_proc=4\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f6f353e24a2a491ca2bdd52dd9c05119",
            "0b49255628fe4ba3a6d085b0be67b436",
            "cec7e95e63ee4fea948e0d035f695801",
            "3b76028e550b47bc865cd555e2e9ba34",
            "53881b7fa80642faacac840211d6bf91",
            "3d03e5df0911476b92206b625b6b4ec5",
            "8afa566b6ae44ae58d23a73eee13d3d8",
            "12075c160c28450faf18fd58ec4cce97",
            "e6896a8f51b54dfb99871bc3173dc008",
            "68957538ca6341018d702374a403c376",
            "4d6be90089464bc8a8268cc024f58bb2"
          ]
        },
        "id": "9rcaC_3eHnMg",
        "outputId": "01300cc0-433c-4f36-df47-70e546bc88f8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/125107 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6f353e24a2a491ca2bdd52dd9c05119"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To re-iterate the cell below modies the above inputs to look like:\n",
        "```\n",
        "'input_ids':  [\n",
        "[list of 1024 tokens],\n",
        "[list of 1024 tokens],  \n",
        "[list of 1024 tokens],\n",
        "...]\n",
        "```\n",
        "\n",
        "Another thing we did is add a label (aka target) to our dataset. The cool thing about CLMs is that you don't really need to do anything special to create a target. Remember our task is to predict the next token; so our target or label is simply the input shifted by one. In our HF models, all we do is copy the inputs, the actual shift happens internally."
      ],
      "metadata": {
        "id": "0ANQflQi0hbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(lm_datasets['train']['input_ids'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6YM3czEHxCY",
        "outputId": "b6048786-4556-431f-8a9e-2421e4e3739d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "Ok our data prep is done, we are ready to start training the model."
      ],
      "metadata": {
        "id": "HHky5-o--Xud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Define your output directory for logs, and the final model\n",
        "OUTPUT_DIR = \"snoop-shpear-clm-v13\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    # Core Training Parameters\n",
        "    per_device_train_batch_size=8,        # how many sequences of 1024 tokens to take at once (more is better if your GPU can handle it)\n",
        "    per_device_eval_batch_size=8,         # Same as train batch size\n",
        "    num_train_epochs=3,                   # How many times do you want to process the dataset, lower values lead to underfitting and higher to over fitting.\n",
        "    learning_rate=5e-5,                   # We are using a pretrained model so our steps should be tiny\n",
        "    weight_decay=0.01,                    # 0.01 is what I found seems to be recommended for finetuning\n",
        "    gradient_accumulation_steps=8,\n",
        "\n",
        "    # Evaluation and Logging\n",
        "    eval_strategy=\"epoch\",                # Evaluation is done at the end of each epoch\n",
        "    logging_steps=50,                     # how often do you want to get updated\n",
        "    save_strategy=\"epoch\",                # Save a checkpoint at the end of each epoch\n",
        "    load_best_model_at_end=True,          # Load the model with the best validation loss\n",
        "\n",
        "    report_to=\"none\",           # by default HF seems to log to wand.ai, I don't want that.\n",
        "\n",
        "    # Resource Management\n",
        "    fp16=True if torch.cuda.is_available() else False,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "gBnfPGtnIjVi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainingArguments sets up parameter for our model. There are a lot of arguments here but pretty much everything I've used is based on standard recommendations from HF."
      ],
      "metadata": {
        "id": "l_mk1AIL-fYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False # Set to False for Causal Language Modeling (CLM)\n",
        ")"
      ],
      "metadata": {
        "id": "UhWUAONKIrKZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Data collators in HF is usually the last step before you can train a model. It handles padding any creating any final labels for processing. It will even covert our inputs to pytorch tensors. For CLM, HF recommends using [DataCollatorForLanguageModeling](https://huggingface.co/docs/transformers/v4.57.1/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling) with mlm=False. The mlm field is True by default and is used for masking tasks."
      ],
      "metadata": {
        "id": "p5n7u3pO9YNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=lm_datasets[\"valid\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8etWrNfzIvYE",
        "outputId": "982b25d2-f521-4b3c-d45b-1264ecffd180"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3375490924.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alrite, lets setup our Trainer. The Trainer abstracts away a lot of the details like using a GPU if available, setting up Optimizers and automatically updating learning rates. It also managers the Pytorch loop around calculating loss, and updating parameters in each step. The TL;DR is that it greatly simplifies training a model vs doing it by hand in Pytorch."
      ],
      "metadata": {
        "id": "q-6ObvBt_As_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n*** Training Complete! ***\")\n",
        "\n",
        "print(train_result.metrics)\n",
        "\n",
        "# Save the final model and tokenizer to your output directory\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(trainer.args.output_dir)\n",
        "print(f\"Model saved to: {trainer.args.output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "wq5D-Oc7I1gE",
        "outputId": "183b8538-c870-4446-a38e-8146b67595b0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [144/144 04:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.075469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.325400</td>\n",
              "      <td>3.910871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.674600</td>\n",
              "      <td>3.905685</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** Training Complete! ***\n",
            "{'train_runtime': 254.6265, 'train_samples_per_second': 35.817, 'train_steps_per_second': 0.566, 'total_flos': 1.002710256058368e+16, 'train_loss': 3.770896805657281, 'epoch': 3.0}\n",
            "Model saved to: snoop-shpear-clm-v13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play with the trained model"
      ],
      "metadata": {
        "id": "D3UCSRgA_lTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "load_from_local = True\n",
        "\n",
        "# Load our saved\n",
        "if load_from_local == True:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "  model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR)\n",
        "else:\n",
        "  # Load from remote\n",
        "  HUB_REPO_ID = \"sagsan/snoopshpear\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(HUB_REPO_ID)\n",
        "  model = AutoModelForCausalLM.from_pretrained(HUB_REPO_ID)\n",
        "\n",
        "# Create the text generation pipeline\n",
        "generator = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhnFGaiSI4Ww",
        "outputId": "8532b143-9046-4484-d92f-8e8b5853cfe9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a serious, formal, 19th-century lawyer. Do not use slang.\"\n",
        "\n",
        "def format_qwen_prompt(user_prompt):\n",
        "    chat_prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n",
        "    chat_prompt += f\"<|im_start|>user\\n{user_prompt}<|im_end|>\\n\"\n",
        "    chat_prompt += \"<|im_start|>assistant\\n\"\n",
        "    print(chat_prompt)\n",
        "    return chat_prompt\n",
        "\n",
        "def generate_text(prompt, max_len=200):\n",
        "\n",
        "    chat_prompt = format_qwen_prompt(prompt)\n",
        "\n",
        "    result = generator(\n",
        "        chat_prompt, # Pass the structured prompt\n",
        "\n",
        "        # Use max_new_tokens to limit the *generated* part\n",
        "        max_new_tokens=max_len,\n",
        "\n",
        "        # --- Your Custom Parameters ---\n",
        "        num_return_sequences=2,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        temperature=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.9,\n",
        "\n",
        "\n",
        "    )\n",
        "    # The output is a list of dicts, extract the generated text\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "\n",
        "    # 3. Clean the Output: Isolate the Model's Response\n",
        "    # Find the position right after the assistant's turn starts\n",
        "    response_start_marker = \"<|im_start|>assistant\\n\"\n",
        "    start_index = generated_text.rfind(response_start_marker)\n",
        "\n",
        "    if start_index != -1:\n",
        "        # Slice the string to get only the generated content\n",
        "        response = generated_text[start_index + len(response_start_marker):]\n",
        "\n",
        "        print(response)\n",
        "        # Remove any trailing stop tokens if they were generated\n",
        "        if response.strip().endswith('<|im_end|>'):\n",
        "             response = response.strip().removesuffix('<|im_end|>')\n",
        "\n",
        "        return response.strip()\n",
        "    else:\n",
        "        # Fallback if the template structure was unexpectedly missing\n",
        "        return \"Error: Could not parse model response.\"\n",
        "\n",
        "\n",
        "# Example 1: The Royal G-Funk Decree\n",
        "prompt1 = \"Fo shizzle, my brethren, I decree that \"\n",
        "print(f\"Prompt: {prompt1}\")\n",
        "print(f\"Output: {generate_text(prompt1, 50)}\\n\")\n",
        "\n",
        "# Example 2: The Philosophical Ride\n",
        "prompt2 = \"To be or not to be, that is the question, \"\n",
        "print(f\"Prompt: {prompt2}\")\n",
        "print(f\"Output: {generate_text(prompt2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT3DN9NgI79S",
        "outputId": "da8be704-abc9-44be-a363-195a1cad1adc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Fo shizzle, my brethren, I decree that \n",
            "<|im_start|>system\n",
            "You are a serious, formal, 19th-century lawyer. Do not use slang.<|im_end|>\n",
            "<|im_start|>user\n",
            "Fo shizzle, my brethren, I decree that <|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Now get thee hence; for here comes the Lady,\n",
            "I'm in your neighborhood and you know it's Doggystyle.\n",
            "When they see me on TV...\n",
            "Thee to another place than this is done now;\n",
            "A little something from hell?\n",
            "Output: Now get thee hence; for here comes the Lady,\n",
            "I'm in your neighborhood and you know it's Doggystyle.\n",
            "When they see me on TV...\n",
            "Thee to another place than this is done now;\n",
            "A little something from hell?\n",
            "\n",
            "Prompt: To be or not to be, that is the question, \n",
            "<|im_start|>system\n",
            "You are a serious, formal, 19th-century lawyer. Do not use slang.<|im_end|>\n",
            "<|im_start|>user\n",
            "To be or not to be, that is the question, <|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "I have found some in their hearts which they speak of,\n",
            "This place where I am now.\n",
            "If you will go with me; but from henceforth let it appear thus: That thou wast born for my hand;\n",
            "Lest your father's wrathful fury come back again!\n",
            "The King himself and all his power stand upon trial here:\n",
            "SNOOP DOGG LYRICS\n",
            "Thou art more than thy name deserves!-My Lord Protector-\n",
            "It’s time we did just what he told us too cuz..\n",
            "He shall find favour amongst them if ever meet him about this wood This way must our course lie till Caesar return home.' A most pitiful farewell-that should make any heart wane By heaven 'tis mine own good fortune To say so would betray an honest man If she were false-born her face was fairer then thee Who comes? Your brother Edward?\n",
            "So much as could do ill on't.- O sweet Hero indeed!, How goes these affairs abroad among th' enemies o'\n",
            "And\n",
            "Output: I have found some in their hearts which they speak of,\n",
            "This place where I am now.\n",
            "If you will go with me; but from henceforth let it appear thus: That thou wast born for my hand;\n",
            "Lest your father's wrathful fury come back again!\n",
            "The King himself and all his power stand upon trial here:\n",
            "SNOOP DOGG LYRICS\n",
            "Thou art more than thy name deserves!-My Lord Protector-\n",
            "It’s time we did just what he told us too cuz..\n",
            "He shall find favour amongst them if ever meet him about this wood This way must our course lie till Caesar return home.' A most pitiful farewell-that should make any heart wane By heaven 'tis mine own good fortune To say so would betray an honest man If she were false-born her face was fairer then thee Who comes? Your brother Edward?\n",
            "So much as could do ill on't.- O sweet Hero indeed!, How goes these affairs abroad among th' enemies o'\n",
            "And\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Publish the model"
      ],
      "metadata": {
        "id": "6VgiElXEop5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"sagsan/snoopshpear\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "tokenizer.push_to_hub(repo_id)\n",
        "\n",
        "\n",
        "model.push_to_hub(\n",
        "    repo_id,\n",
        "    commit_message=\"updated with newlines\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "aBpno5HnopK5"
      },
      "execution_count": 37,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bea416a2c40442b99117d3f69a75ff7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f133cfddc0d14e3688f6e1a400902c4b",
              "IPY_MODEL_9fd55bfb36d5494abb15302785790228",
              "IPY_MODEL_f20b9c0fdc7b48c39b879e6ed923d271"
            ],
            "layout": "IPY_MODEL_f3a8c1ff7b2c4fd5a3c786509418946e"
          }
        },
        "f133cfddc0d14e3688f6e1a400902c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8078745300a14a5bb63be165a8ab7cc8",
            "placeholder": "​",
            "style": "IPY_MODEL_cf2c50affcca4aeeb1e88c28f41b355e",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "9fd55bfb36d5494abb15302785790228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42871a9383c34b4dad6ae773da0e4395",
            "max": 125107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9699df0602be49798cbc5e2298ef2f36",
            "value": 125107
          }
        },
        "f20b9c0fdc7b48c39b879e6ed923d271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27abf1288494497ca25323408ee2fe57",
            "placeholder": "​",
            "style": "IPY_MODEL_10992af4c59a4b2980cfab1e3ee2bef9",
            "value": " 125107/125107 [00:01&lt;00:00, 83074.46 examples/s]"
          }
        },
        "f3a8c1ff7b2c4fd5a3c786509418946e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8078745300a14a5bb63be165a8ab7cc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf2c50affcca4aeeb1e88c28f41b355e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42871a9383c34b4dad6ae773da0e4395": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9699df0602be49798cbc5e2298ef2f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27abf1288494497ca25323408ee2fe57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10992af4c59a4b2980cfab1e3ee2bef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04b7a82085bb4f0ca7c8f530951d1588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eedf977a3a504e8a9f228f6c090413bd",
              "IPY_MODEL_629a2b25a7ac450aa946688dc0fe1e51",
              "IPY_MODEL_68bc957dfa174910ad233ab5acf4ad4b"
            ],
            "layout": "IPY_MODEL_36cb3b3cb38341879912e21138243baa"
          }
        },
        "eedf977a3a504e8a9f228f6c090413bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b5f9fae9f484756b09b8f3d121c7e83",
            "placeholder": "​",
            "style": "IPY_MODEL_b00e1d6b517d40428e75b4b56bef878d",
            "value": "Map: 100%"
          }
        },
        "629a2b25a7ac450aa946688dc0fe1e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e35dd3d400a043dca8a4524a7a5cb5dc",
            "max": 125107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b6f4e94583940678657c8665cba6d60",
            "value": 125107
          }
        },
        "68bc957dfa174910ad233ab5acf4ad4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab056cbae56f435387f837cc674c7620",
            "placeholder": "​",
            "style": "IPY_MODEL_126cb3716c0244e3a79973be5ed777e9",
            "value": " 125107/125107 [00:07&lt;00:00, 18082.94 examples/s]"
          }
        },
        "36cb3b3cb38341879912e21138243baa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b5f9fae9f484756b09b8f3d121c7e83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b00e1d6b517d40428e75b4b56bef878d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e35dd3d400a043dca8a4524a7a5cb5dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b6f4e94583940678657c8665cba6d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab056cbae56f435387f837cc674c7620": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "126cb3716c0244e3a79973be5ed777e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6f353e24a2a491ca2bdd52dd9c05119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b49255628fe4ba3a6d085b0be67b436",
              "IPY_MODEL_cec7e95e63ee4fea948e0d035f695801",
              "IPY_MODEL_3b76028e550b47bc865cd555e2e9ba34"
            ],
            "layout": "IPY_MODEL_53881b7fa80642faacac840211d6bf91"
          }
        },
        "0b49255628fe4ba3a6d085b0be67b436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d03e5df0911476b92206b625b6b4ec5",
            "placeholder": "​",
            "style": "IPY_MODEL_8afa566b6ae44ae58d23a73eee13d3d8",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "cec7e95e63ee4fea948e0d035f695801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12075c160c28450faf18fd58ec4cce97",
            "max": 125107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6896a8f51b54dfb99871bc3173dc008",
            "value": 125107
          }
        },
        "3b76028e550b47bc865cd555e2e9ba34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68957538ca6341018d702374a403c376",
            "placeholder": "​",
            "style": "IPY_MODEL_4d6be90089464bc8a8268cc024f58bb2",
            "value": " 125107/125107 [00:02&lt;00:00, 54141.61 examples/s]"
          }
        },
        "53881b7fa80642faacac840211d6bf91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d03e5df0911476b92206b625b6b4ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8afa566b6ae44ae58d23a73eee13d3d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12075c160c28450faf18fd58ec4cce97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6896a8f51b54dfb99871bc3173dc008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68957538ca6341018d702374a403c376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d6be90089464bc8a8268cc024f58bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}